Lots of negative comments on Docker
- "Containers don't contain"
  https://opensource.com/business/14/7/docker-security-selinux

- "... total systematic failure of all logic related to image security"
  https://titanous.com/posts/docker-insecurity

- The shared surface between applications is now the kernel
- Denial of service of resources (cpu, memory, storage)
- Poisoned images
- Secrets management?


- Container breakouts are treated as unlikely but possible, cgroups and namespaces are relatively new -- we're probably going to see exploits
    - Use VMs to segregate groups of containers

- Docker privileges == root privileges
- If you're using the Docker remote API, only allow access via TLS with authenticate client on AND only listen on internal network/VPN

- Users are not namespaced (yet!), root in container == root on host
- Don't run your applications as root:
    RUN groupadd -r nobody && useradd -r -g nobody nobody
    USER nobody

- Set container FS to read-only
    docker run --read-only debian bash touch /test

- Set bind mounted volumes to read-only:
    docker run -v $(pwd):/data debian touch /data/test

- SELinux/AppArmor

- Set cpu shares
    docker run -d myimage
    docker run -d -c 512 myimage

- set memory limits
    docker run -m 512m myimage

- Disable inter-container communication (but retain link communication)
    docker daemon --icc="false" --iptables="true"

- Using Docker Hub images
    - Only use images that come from someone you trust
    - Automated builds and linked Dockerfile that you can inspect
    - Docker pull by digest:
        docker pull debian@....

- Package vulnerabilities & system binaries
    $ find / -perm +6000 -type f -exec ls -ld {} \;
    - Run a minimal distribution to reduce attack surface

- The real issue is application vulnerabilities

- Investigation of an attack
    $ docker diff my_container
    .. Okay, we see something changed?
    $ docker commit my_container
    abcdef1234567890
    $ docker kill my_container
    $ docker run new_container
    $ docker run abcdef1234567890 /bin/bash


- Secrets
    - Bake into image: NO!
    - Environment variables -- can inspect them
    - Volumes -- too much hassle
    - KV store
    

---

 - The need for an image signing framework
 - The Update Framework (TUF) was invented in 2009 by a bunch of researchers
 - Notary is an open source tool that Docker launched at DockerCon which is an opinionated implementation of TUF
 - Docker Content Trust is an opinionated implementation of Notary

Why not GPG?
- Unfair comparison to GPG
- GPG is a message format, it's not meant to provide security on its own
- Can't simply apply GPG to RPM signing
 - We sign an RPM with a key, expiration 10 years
 - We distribute software, everything is fine -- servers verify the image
 - 2 years later, an attacker gains a man-in-the middle position, when one of your hosts attempts to update a package -- they're going to serve you a valid signed but vulnerable package from 2 years ago -- that has shellshock (replay attack)
- If the GPG key is comprimsed, we have to go to all clients and bootstrap new keys
- Lots more to securing distribution of software than just signing content

"A software update system is secure if it can be sure that it knows about the latest available updates in a timely manner, and any files it downs are the correct files, and no harm results from checking or downloading files".
- The Update Framework

The Update Framework (TUF)
- Developed for Tor to protect against nation state attackers
- Best standard for secure software distribution that we have at the moment
Core concepts
 - Freshness: 
   - everything has an expiration to resist against replay attacks
   - guarantee that not only is the content signed, but it is the latest version of the software
 - Online vs. Offline keys:
   - Online key equivalent to GPG key, but inherently more vulnerable since it has to be online signing packages
   - Offline key that's kept in vault/bank/secure location, used for key rotation
   - This allows us to survive key comprimise

 - Signed collections instead of individual files:
   - Traditional signing is done on a file/package basis, TUF instead uses collections.  So we can think of it as a snapshot of files that will be distributed to clients and they shouldn't see anything else.  This defends against package mismatch attacks. 

- TUF Internals
  - Four different keys, ranging from short to long expirations:
    - Timestamp key (kept online)
    - Snapshot key
    - Targets key
    - Root key

  - Comprimisation of timestamp key means that you lose freshness guarantees, but an attacker still cannot sign a package as if they were you
  - Snapshots key
  - Targets equivalent of GPG key (workhorse), thing that's online signing these files -- kept online.
  - Root key is kept offline and sole objective is to sign the other three keys


- Notary is an opinionated implementation of TUF
  - Tries to abstract and simplify the framework a little bit
  - 2 servers
    - Notary signer, online but not on the front-end
    - Notary server, front-end only has public data
